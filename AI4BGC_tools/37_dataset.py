#!/usr/bin/env python3
import os
import sys
import glob
import argparse
from config import Config
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from scipy.spatial import cKDTree
import netCDF4 as nc


def build_restart_kdtree(ds_restart: nc.Dataset) -> Tuple[cKDTree, np.ndarray]:
    gridcell_lat = ds_restart.variables["grid1d_lat"][:]
    gridcell_lon = ds_restart.variables["grid1d_lon"][:]
    coords = np.vstack((gridcell_lat, gridcell_lon)).T
    tree = cKDTree(coords)
    return tree, coords


def build_column_index_map(ds_restart: nc.Dataset) -> Dict[int, np.ndarray]:
    cols1d_gridcell_index = ds_restart.variables["cols1d_gridcell_index"][:]
    unique_ids = np.unique(cols1d_gridcell_index)
    mapping: Dict[int, np.ndarray] = {}
    for grid_id in unique_ids:
        mapping[int(grid_id)] = np.where(cols1d_gridcell_index == grid_id)[0]
    return mapping


def ensure_vars_exist(ds: nc.Dataset, var_names: List[str]) -> List[str]:
    existing = []
    for name in var_names:
        if name in ds.variables:
            existing.append(name)
    return existing


def build_pft_index_map(ds_restart: nc.Dataset) -> Dict[int, np.ndarray]:
    pfts1d_gridcell_index = ds_restart.variables["pfts1d_gridcell_index"][:]
    unique_ids = np.unique(pfts1d_gridcell_index)
    mapping: Dict[int, np.ndarray] = {}
    for grid_id in unique_ids:
        mapping[int(grid_id)] = np.where(pfts1d_gridcell_index == grid_id)[0]
    return mapping


def extract_col1d_x(ds_restart: nc.Dataset, var_name: str, col_indices: np.ndarray) -> List[float]:
    if col_indices.size == 0:
        return []
    values = ds_restart.variables[var_name][col_indices]
    return values.astype(float).tolist()


def extract_col1d_y(ds_r_list: List[nc.Dataset], var_name: str, col_indices: np.ndarray) -> List[float]:
    if col_indices.size == 0:
        return []
    slices: List[np.ndarray] = []
    for ds_r in ds_r_list:
        values = ds_r.variables[var_name][col_indices]
        slices.append(np.asarray(values, dtype=float))
    stacked = np.stack(slices, axis=0)  # (n_files, n_cols_for_cell)
    avg = np.mean(stacked, axis=0)
    return avg.tolist()


def extract_col2d_x(ds_restart: nc.Dataset, var_name: str, col_indices: np.ndarray) -> List[List[float]]:
    """æå–åˆ—å°ºåº¦ 2D å˜é‡ï¼ˆå½¢å¦‚ [n_cols, n_layers]ï¼‰çš„ X å€¼åˆ—è¡¨ã€‚"""
    if col_indices.size == 0:
        return []
    values = ds_restart.variables[var_name][col_indices, :]
    return np.asarray(values, dtype=float).tolist()


def extract_col2d_y(ds_r_list: List[nc.Dataset], var_name: str, col_indices: np.ndarray) -> List[List[float]]:
    """ä»å¤šä¸ª Y æ–‡ä»¶ä¸­æå–åˆ—å°ºåº¦ 2D å˜é‡å¹¶æŒ‰æ–‡ä»¶ç»´åº¦æ±‚å¹³å‡ï¼Œè¿”å› [n_cols, n_layers] åˆ—è¡¨ã€‚"""
    if col_indices.size == 0:
        return []
    slices: List[np.ndarray] = []
    for ds_r in ds_r_list:
        values = ds_r.variables[var_name][col_indices, :]
        slices.append(np.asarray(values, dtype=float))
    stacked = np.stack(slices, axis=0)  # (n_files, n_cols_for_cell, n_layers)
    avg = np.mean(stacked, axis=0)
    return avg.tolist()


def extract_pft1d_x(ds_restart: nc.Dataset, var_name: str, pft_indices: np.ndarray) -> List[float]:
    if pft_indices.size == 0:
        return []
    values = ds_restart.variables[var_name][pft_indices]
    return np.asarray(values, dtype=float).tolist()


def extract_pft1d_y(ds_r_list: List[nc.Dataset], var_name: str, pft_indices: np.ndarray) -> List[float]:
    if pft_indices.size == 0:
        return []
    slices: List[np.ndarray] = []
    for ds_r in ds_r_list:
        values = ds_r.variables[var_name][pft_indices]
        slices.append(np.asarray(values, dtype=float))
    stacked = np.stack(slices, axis=0)  # (n_files, n_pfts_for_cell)
    avg = np.mean(stacked, axis=0)
    return avg.tolist()


def extract_pft2d_x(ds_restart: nc.Dataset, var_name: str, pft_indices: np.ndarray) -> List[List[float]]:
    if pft_indices.size == 0:
        return []
    values = ds_restart.variables[var_name][pft_indices, :]
    return np.asarray(values, dtype=float).tolist()


def extract_pft2d_y(ds_r_list: List[nc.Dataset], var_name: str, pft_indices: np.ndarray) -> List[List[float]]:
    if pft_indices.size == 0:
        return []
    slices: List[np.ndarray] = []
    for ds_r in ds_r_list:
        values = ds_r.variables[var_name][pft_indices, :]
        slices.append(np.asarray(values, dtype=float))
    stacked = np.stack(slices, axis=0)  # (n_files, n_pfts_for_cell, n_layers)
    avg = np.mean(stacked, axis=0)
    return avg.tolist()


def augment_dataframe_with_pools(
    df: pd.DataFrame,
    ds_restart: nc.Dataset,
    ds_r_list: List[nc.Dataset],
    restart_tree: cKDTree,
    restart_coords: np.ndarray,
    col_index_map: Dict[int, np.ndarray],
    pool_vars: List[str],
) -> pd.DataFrame:
    # è¿‡æ»¤æ‰ç¼ºå°‘åæ ‡çš„æ•°æ®
    if "Latitude" not in df.columns or "Longitude" not in df.columns:
        raise ValueError("DataFrame ç¼ºå°‘ Latitude/Longitude åˆ—ï¼Œæ— æ³•æ˜ å°„åˆ° restart ç½‘æ ¼ã€‚")

    # ä»…ä¿ç•™å­˜åœ¨äºæ•°æ®é›†ä¸­çš„å˜é‡
    pool_vars_existing = ensure_vars_exist(ds_restart, pool_vars)
    if not pool_vars_existing:
        raise ValueError(f"åœ¨ restart æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡å˜é‡: {pool_vars}")

    # åŒæ—¶è¦æ±‚è¿™äº›å˜é‡åœ¨ Y ç«¯æ–‡ä»¶ä¸­ä¹Ÿå­˜åœ¨
    pool_vars_final: List[str] = []
    for v in pool_vars_existing:
        if all(v in ds_r.variables for ds_r in ds_r_list):
            pool_vars_final.append(v)
        else:
            print(f"[è­¦å‘Š] å˜é‡ {v} åœ¨æŸäº› Y æ–‡ä»¶ä¸­ç¼ºå¤±ï¼Œè·³è¿‡è¯¥å˜é‡ã€‚")

    if not pool_vars_final:
        raise ValueError("ç›®æ ‡å˜é‡åœ¨ Y æ–‡ä»¶ç»„ä¸­å‡ä¸å­˜åœ¨ã€‚")

    latitudes = df["Latitude"].to_numpy()
    longitudes = df["Longitude"].to_numpy()
    query_coords = np.vstack((latitudes, longitudes)).T

    # æœ€è¿‘é‚» gridcellï¼ˆrestart ç½‘æ ¼ï¼‰ç´¢å¼•
    _, nearest_restart_indices = restart_tree.query(query_coords, k=1)

    # ä¸ºæ¯ä¸ªè§‚æµ‹è¡Œè¿½åŠ å››ä¸ªå˜é‡åŠå…¶ Y_
    results_x: Dict[str, List[List[float]]] = {v: [] for v in pool_vars_final}
    results_y: Dict[str, List[List[float]]] = {f"Y_{v}": [] for v in pool_vars_final}

    for row_idx, restart_idx in enumerate(nearest_restart_indices):
        gridcell_id = int(restart_idx) + 1  # æ–‡ä»¶ä¸­çš„ç´¢å¼•é€šå¸¸ä» 1 å¼€å§‹
        col_indices = col_index_map.get(gridcell_id, np.array([], dtype=int))

        for v in pool_vars_final:
            x_vals = extract_col1d_x(ds_restart, v, col_indices)
            y_vals = extract_col1d_y(ds_r_list, v, col_indices)

            results_x[v].append(x_vals)
            results_y[f"Y_{v}"].append(y_vals)

        if (row_idx + 1) % 1000 == 0:
            print(f"  å·²å¤„ç† {row_idx + 1} / {len(df)} è¡Œ ...")

    # å†™å…¥ DataFrame
    for v in pool_vars_final:
        df[v] = results_x[v]
        df[f"Y_{v}"] = results_y[f"Y_{v}"]

    return df


def augment_dataframe_with_vars(
    df: pd.DataFrame,
    ds_restart: nc.Dataset,
    ds_special_p_restart: nc.Dataset, # <--- ADDED
    ds_r_list: List[nc.Dataset],
    restart_tree: cKDTree,
    restart_coords: np.ndarray,
    col_index_map: Dict[int, np.ndarray],
    pft_index_map: Dict[int, np.ndarray],
    vars_1d: List[str],
    vars_2d: List[str],
    special_p_vars: List[str], # <--- ADDED
    
) -> pd.DataFrame:
    """
    å°†æ¥è‡ª restart ä¸ Y æ–‡ä»¶ç»„çš„åˆ—å°ºåº¦ 1D/2D å˜é‡ï¼ˆåç§°æ¥è‡ªé…ç½®æˆ– txt è§£æï¼‰è¿½åŠ åˆ° DataFrameã€‚
    - å¯¹ 1D å˜é‡ï¼šæå–å½¢å¦‚ [n_cols] çš„å€¼ï¼›å¯¹ Yï¼šåœ¨æ–‡ä»¶ç»´åº¦æ±‚å¹³å‡åå¾—åˆ° [n_cols]
    - å¯¹ 2D å˜é‡ï¼šæå–å½¢å¦‚ [n_cols, n_layers] çš„å€¼ï¼›å¯¹ Yï¼šåœ¨æ–‡ä»¶ç»´åº¦æ±‚å¹³å‡åå¾—åˆ° [n_cols, n_layers]
    è¿”å›å¢å¼ºåçš„ DataFrameã€‚
    """
    if "Latitude" not in df.columns or "Longitude" not in df.columns:
        raise ValueError("DataFrame ç¼ºå°‘ Latitude/Longitude åˆ—ï¼Œæ— æ³•æ˜ å°„åˆ° restart ç½‘æ ¼ã€‚")

    # å˜é‡å­˜åœ¨æ€§ç­›é€‰ï¼ˆè¦æ±‚åœ¨ restart ä¸æ‰€æœ‰ Y æ–‡ä»¶ä¸­å‡å­˜åœ¨ï¼‰
    vars_1d_existing = ensure_vars_exist(ds_restart, vars_1d)
    vars_2d_existing = ensure_vars_exist(ds_restart, vars_2d)
    
    final_1d: List[str] = [v for v in vars_1d_existing if all(v in ds_r.variables for ds_r in ds_r_list)]
    final_2d: List[str] = [v for v in vars_2d_existing if all(v in ds_r.variables for ds_r in ds_r_list)]

    if not final_1d and not final_2d:
        print("è­¦å‘Š: ç›®æ ‡ 1D/2D å˜é‡åœ¨ restart æˆ– Y æ–‡ä»¶ç»„ä¸­å‡ä¸å­˜åœ¨ã€‚")
        return df
    # ========================== æ–°å¢çš„æ‰“å°é€»è¾‘å¼€å§‹ ==========================
    print("\n" + "="*70)
    print("ğŸ”¬ å˜é‡åˆ†ç±»æŠ¥å‘Š (åŸºäºNetCDFç»´åº¦è‡ªåŠ¨è¯†åˆ«)")
    print("="*70)

    classified_pft_1d, classified_col_1d = [], []
    classified_pft_2d, classified_col_2d = [], []

    # å¯¹1Då˜é‡è¿›è¡Œé¢„æ‰«æå’Œåˆ†ç±»
    for v in final_1d:
        var_obj = ds_restart.variables[v]
        if "pft" in tuple(var_obj.dimensions):
            classified_pft_1d.append(v)
        else:
            classified_col_1d.append(v)

    # å¯¹2Då˜é‡è¿›è¡Œé¢„æ‰«æå’Œåˆ†ç±»
    for v in final_2d:
        ds_for_x = ds_special_p_restart if v in special_p_vars else ds_restart
        var_obj = ds_for_x.variables[v]
        if "pft" in tuple(var_obj.dimensions):
            classified_pft_2d.append(v)
        else:
            classified_col_2d.append(v)

    # æ‰“å°æ ¼å¼åŒ–çš„æŠ¥å‘Š
    print("â–¶ï¸  è¯†åˆ«ä¸º [PFT å˜é‡] (1D):")
    print(f"   {classified_pft_1d if classified_pft_1d else '(æ— )'}")

    print("\nâ–¶ï¸  è¯†åˆ«ä¸º [Column å˜é‡] (1D):")
    print(f"   {classified_col_1d if classified_col_1d else '(æ— )'}")

    print("\nâ–¶ï¸  è¯†åˆ«ä¸º [PFT å˜é‡] (2D):")
    print(f"   {classified_pft_2d if classified_pft_2d else '(æ— )'}")

    print("\nâ–¶ï¸  è¯†åˆ«ä¸º [Column å˜é‡] (2D):")
    print(f"   {classified_col_2d if classified_col_2d else '(æ— )'}")

    print("="*70 + "\n")
    # ========================== æ–°å¢çš„æ‰“å°é€»è¾‘ç»“æŸ ==========================

    final_1d: List[str] = []
    for v in vars_1d_existing:
        if all(v in ds_r.variables for ds_r in ds_r_list):
            final_1d.append(v)
        else:
            print(f"[è­¦å‘Š] å˜é‡ {v} åœ¨æŸäº› Y æ–‡ä»¶ä¸­ç¼ºå¤±ï¼Œè·³è¿‡è¯¥å˜é‡ã€‚")

    final_2d: List[str] = []
    for v in vars_2d_existing:
        if all(v in ds_r.variables for ds_r in ds_r_list):
            final_2d.append(v)
        else:
            print(f"[è­¦å‘Š] å˜é‡ {v} åœ¨æŸäº› Y æ–‡ä»¶ä¸­ç¼ºå¤±ï¼Œè·³è¿‡è¯¥å˜é‡ã€‚")

    if not final_1d and not final_2d:
        print("è­¦å‘Š: ç›®æ ‡ 1D/2D å˜é‡åœ¨ restart æˆ– Y æ–‡ä»¶ç»„ä¸­å‡ä¸å­˜åœ¨ã€‚")
        return df # <--- MODIFIED: å¦‚æœæ²¡æœ‰å˜é‡å¯å¤„ç†ï¼Œç›´æ¥è¿”å›åŸå§‹df
    # ========== ä»è¿™é‡Œç²˜è´´æ–°å¢çš„ä»£ç  ==========
    print("\n" + "="*50)
    print(f"[ç¡®è®¤] å¼€å§‹ä¸ºæ‰¹æ¬¡æ•°æ®å¢å¼ºå˜é‡ï¼Œä»¥ä¸‹æ˜¯Xå€¼çš„æ¥æºæ–‡ä»¶ç¡®è®¤ï¼š")
    
    default_x_source_file = ds_restart.filepath()
    special_x_source_file = ds_special_p_restart.filepath()
    
    if final_2d:
        print("\n--- 2Då˜é‡æ¥æº ---")
        for var_name in sorted(final_2d):
            if var_name in special_p_vars:
                print(f"  -> â­ å˜é‡ '{var_name}':  å°†ä»ç‰¹æ®Šæ–‡ä»¶è¯»å– \n\t    '{special_x_source_file}'")
            else:
                print(f"  ->   å˜é‡ '{var_name}':  å°†ä»é»˜è®¤æ–‡ä»¶è¯»å– \n\t    '{default_x_source_file}'")
    
    if final_1d:
        print("\n--- 1Då˜é‡æ¥æº ---")
        for var_name in sorted(final_1d):
            print(f"  ->   å˜é‡ '{var_name}':  å°†ä»é»˜è®¤æ–‡ä»¶è¯»å– \n\t    '{default_x_source_file}'")

    print("="*50 + "\n")
    # ========== æ–°å¢ä»£ç ç²˜è´´ç»“æŸ ==========
    latitudes = df["Latitude"].to_numpy()
    longitudes = df["Longitude"].to_numpy()
    query_coords = np.vstack((latitudes, longitudes)).T

    # æœ€è¿‘é‚» gridcellï¼ˆrestart ç½‘æ ¼ï¼‰ç´¢å¼•
    _, nearest_restart_indices = restart_tree.query(query_coords, k=1)

    # ç»“æœå®¹å™¨
    results_x_1d: Dict[str, List[List[float]]] = {v: [] for v in final_1d}
    results_y_1d: Dict[str, List[List[float]]] = {f"Y_{v}": [] for v in final_1d}
    results_x_2d: Dict[str, List[List[List[float]]]] = {v: [] for v in final_2d}
    results_y_2d: Dict[str, List[List[List[float]]]] = {f"Y_{v}": [] for v in final_2d}

    for row_idx, restart_idx in enumerate(nearest_restart_indices):
        gridcell_id = int(restart_idx) + 1  # æ–‡ä»¶ä¸­çš„ç´¢å¼•é€šå¸¸ä» 1 å¼€å§‹
        col_indices = col_index_map.get(gridcell_id, np.array([], dtype=int))
        pft_indices = pft_index_map.get(gridcell_id, np.array([], dtype=int))

        for v in final_1d:
            var_obj = ds_restart.variables[v]
            dims = tuple(var_obj.dimensions)
            if "pft" in dims:
                x_vals = extract_pft1d_x(ds_restart, v, pft_indices)
                y_vals = extract_pft1d_y(ds_r_list, v, pft_indices)
            else:
                x_vals = extract_col1d_x(ds_restart, v, col_indices)
                y_vals = extract_col1d_y(ds_r_list, v, col_indices)
            results_x_1d[v].append(x_vals)
            results_y_1d[f"Y_{v}"].append(y_vals)

        for v in final_2d:
            # åˆ¤æ–­å½“å‰å˜é‡æ˜¯å¦ä¸ºç‰¹æ®ŠPå˜é‡ï¼Œä»¥å†³å®šä»å“ªä¸ªæ–‡ä»¶è¯»å–Xå€¼
            if v in special_p_vars:
                ds_for_x = ds_special_p_restart
            else:
                ds_for_x = ds_restart
            var_obj = ds_for_x.variables[v] # ä½¿ç”¨æ­£ç¡®çš„ ds å¯¹è±¡
            dims = tuple(var_obj.dimensions)
            
            if "pft" in dims:
                x_vals_2d = extract_pft2d_x(ds_for_x, v, pft_indices)
                y_vals_2d = extract_pft2d_y(ds_r_list, v, pft_indices) # Yå€¼æ¥æºä¸å˜
            else:
                x_vals_2d = extract_col2d_x(ds_for_x, v, col_indices)
                y_vals_2d = extract_col2d_y(ds_r_list, v, col_indices) # Yå€¼æ¥æºä¸å˜
            
            # var_obj = ds_restart.variables[v]
            # dims = tuple(var_obj.dimensions)
            # if "pfts1d" in dims:
            #     x_vals_2d = extract_pft2d_x(ds_restart, v, pft_indices)
            #     y_vals_2d = extract_pft2d_y(ds_r_list, v, pft_indices)
            # else:
            #     x_vals_2d = extract_col2d_x(ds_restart, v, col_indices)
            #     y_vals_2d = extract_col2d_y(ds_r_list, v, col_indices)
            results_x_2d[v].append(x_vals_2d)
            results_y_2d[f"Y_{v}"].append(y_vals_2d)

        if (row_idx + 1) % 1000 == 0:
            print(f"  å·²å¤„ç† {row_idx + 1} / {len(df)} è¡Œ ...")

    # å†™å…¥ DataFrame
    for v in final_1d:
        df[v] = results_x_1d[v]
        df[f"Y_{v}"] = results_y_1d[f"Y_{v}"]
    for v in final_2d:
        df[v] = results_x_2d[v]
        df[f"Y_{v}"] = results_y_2d[f"Y_{v}"]

    return df


def main():
    parser = argparse.ArgumentParser(
        description=
        "ä¸ºç°æœ‰ batched è®­ç»ƒæ•°æ®è¿½åŠ æŒ‰åˆ†ç±»æ–°å¢çš„ RESTART å˜é‡åŠå¯¹åº” Y_ åˆ—ï¼ˆå˜é‡åæ¥è‡ª config/txt é…ç½®ï¼‰"
    )
    parser.add_argument(
        "--input_glob",
        default=Config.INPUT_GLOB,
        help="è¾“å…¥ pkl æ‰¹æ¬¡æ–‡ä»¶çš„ glob è·¯å¾„æ¨¡å¼ï¼ˆé»˜è®¤åŸå§‹æ•°æ®è·¯å¾„ä¸æ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--output_dir",
        default=Config.OUTPUT_DIR,
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤å¢å¼ºæ•°æ®ç›®å½•ï¼‰"
    )

    args = parser.parse_args()

    # æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå†™æ­»ä¸ºå›ºå®šè·¯å¾„ï¼Œç‹¬ç«‹è¿è¡Œæ— éœ€ä¾èµ– configï¼‰
    file_path10 = \
        "/home/UNT/dg0997/all_gdw/0_oak_weather/dataset/ornl_data_700/20250117_trendytest_ICB1850CNRDCTCBC_ad_spinup.elm.r.0021-01-01-00000.nc"

    file_path17 = \
        "/home/UNT/dg0997/all_gdw/0_oak_weather/dataset/ornl_data_700/output/20250117_trendytest_ICB1850CNPRDCTCBC.elm.r.0701-01-01-00000.nc"
    file_path18 = \
        "/home/UNT/dg0997/all_gdw/0_oak_weather/dataset/ornl_data_700/output/20250117_trendytest_ICB1850CNPRDCTCBC.elm.r.0721-01-01-00000.nc"
    file_path19 = \
        "/home/UNT/dg0997/all_gdw/0_oak_weather/dataset/ornl_data_700/output/20250117_trendytest_ICB1850CNPRDCTCBC.elm.r.0741-01-01-00000.nc"
    file_path20 = \
        "/home/UNT/dg0997/all_gdw/0_oak_weather/dataset/ornl_data_700/output/20250117_trendytest_ICB1850CNPRDCTCBC.elm.r.0761-01-01-00000.nc"
    file_path21 = \
        "/home/UNT/dg0997/all_gdw/0_oak_weather/dataset/ornl_data_700/output/20250117_trendytest_ICB1850CNPRDCTCBC.elm.r.0781-01-01-00000.nc"

    # ä»é…ç½®è¯»å–æ–°å¢åˆ†ç±»å¹¶æ˜ å°„åˆ°ä¸‰ä¸ª RESTART ç»„ï¼š
    # - RESTART_PFT_VARS        <- dataset_new_1D_PFT_VARIABLES
    # - RESTART_COL_1D_VARS     <- dataset_new_RESTART_COL_1D_VARS
    # - RESTART_COL_2D_VARS     <- dataset_new_Water_variables + dataset_new_2D_VARIABLES
    restart_pft_vars = list(dict.fromkeys(Config.dataset_new_1D_PFT_VARIABLES))
    restart_col_1d_vars = list(dict.fromkeys(Config.dataset_new_RESTART_COL_1D_VARS))
    restart_col_2d_vars = list(dict.fromkeys(list(Config.dataset_new_Water_variables) + list(Config.dataset_new_2D_VARIABLES)))

    # æ˜ å°„åˆ°å¢å¼ºå‡½æ•°éœ€è¦çš„ 1D/2D è¾“å…¥
    # configured_vars_1d = list(dict.fromkeys(restart_pft_vars + restart_col_1d_vars))
    # configured_vars_2d = list(dict.fromkeys(restart_col_2d_vars))
    configured_vars_1d = list(dict.fromkeys(restart_pft_vars + restart_col_1d_vars))
    configured_vars_2d = list(dict.fromkeys(restart_col_2d_vars + Config.SPECIAL_P_VARS))

    input_files = sorted(glob.glob(args.input_glob))
    if not input_files:
        print(f"æœªæ‰¾åˆ°ä»»ä½•è¾“å…¥æ–‡ä»¶: {args.input_glob}")
        sys.exit(1)

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)

    print("æ‰“å¼€ restart æ–‡ä»¶ä¸ Y æ–‡ä»¶ç»„ ...")
    ds_restart = nc.Dataset(file_path10)
    ds_special_p_restart = nc.Dataset(Config.SPECIAL_P_INPUT_NC) 
    ds_r_list = [nc.Dataset(fp) for fp in [file_path17, file_path18, file_path19, file_path20, file_path21]]

    try:
        print("æ„å»º restart ç½‘æ ¼ KDTree ä¸åˆ—ç´¢å¼•æ˜ å°„ ...")
        restart_tree, restart_coords = build_restart_kdtree(ds_restart)
        col_index_map = build_column_index_map(ds_restart)
        pft_index_map = build_pft_index_map(ds_restart)
        
        
        print(f"å¼€å§‹å¤„ç† {len(input_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶ ...")
        for fp in input_files:
            print(f"å¤„ç†: {fp}")
            df = pd.read_pickle(fp)

            # ä»…å¯¹ç¼ºå¤±çš„æ–°å¢å˜é‡è¿›è¡Œå¢å¼ºï¼›è‹¥å…¨éƒ¨å·²å­˜åœ¨åˆ™è·³è¿‡è¯¥æ‰¹æ¬¡
            # æ³¨æ„ï¼šåªé’ˆå¯¹â€œæ–°å¢åˆ†ç±»å˜é‡â€ï¼Œä¸å½±å“æ—§æœ‰åˆ—
            # missing_vars_1d = [v for v in configured_vars_1d if not (v in df.columns and f"Y_{v}" in df.columns)]
            # missing_vars_2d = [v for v in configured_vars_2d if not (v in df.columns and f"Y_{v}" in df.columns)]
            # existing_vars_1d = [v for v in configured_vars_1d if v not in missing_vars_1d]
            # existing_vars_2d = [v for v in configured_vars_2d if v not in missing_vars_2d]

            # if not missing_vars_1d and not missing_vars_2d:
            #     print("  æ–°å¢å˜é‡åœ¨è¯¥æ‰¹æ¬¡å‡å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            #     print(f"  ç›®æ ‡æ–°å¢å˜é‡ï¼ˆ1Dï¼‰: {configured_vars_1d}")
            #     print(f"  ç›®æ ‡æ–°å¢å˜é‡ï¼ˆ2Dï¼‰: {configured_vars_2d}")
            #     print(f"  å·²å­˜åœ¨ï¼ˆ1Dï¼‰: {existing_vars_1d}")
            #     print(f"  å·²å­˜åœ¨ï¼ˆ2Dï¼‰: {existing_vars_2d}")
            #     continue
            # ========== ä»è¿™é‡Œå¼€å§‹å¤åˆ¶ï¼Œæ›¿æ¢ä¸Šé¢çš„æ—§ä»£ç å— ==========

            # 1. è¯†åˆ«éœ€è¦å¼ºåˆ¶æ›¿æ¢çš„å˜é‡ (æ¥è‡ªConfig)
            force_replace_vars = set(Config.SPECIAL_P_VARS)

            # 2. æ‰¾å‡ºé™¤äº†å¼ºåˆ¶æ›¿æ¢å˜é‡ä¹‹å¤–çš„å…¶ä»–ç¼ºå¤±å˜é‡
            missing_other_vars_1d = [
                v for v in configured_vars_1d 
                if v not in force_replace_vars and not (v in df.columns and f"Y_{v}" in df.columns)
            ]
            missing_other_vars_2d = [
                v for v in configured_vars_2d 
                if v not in force_replace_vars and not (v in df.columns and f"Y_{v}" in df.columns)
            ]

            # 3. åˆå¹¶æˆæœ€ç»ˆéœ€è¦å¤„ç†çš„å˜é‡åˆ—è¡¨
            #    - 1Då˜é‡ = å…¶ä»–ç¼ºå¤±çš„1Då˜é‡ (å› ä¸ºç‰¹æ®Šå˜é‡éƒ½æ˜¯2Dçš„)
            #    - 2Då˜é‡ = å…¶ä»–ç¼ºå¤±çš„2Då˜é‡ + æ‰€æœ‰éœ€è¦å¼ºåˆ¶æ›¿æ¢çš„å˜é‡
            vars_to_process_1d = missing_other_vars_1d
            # ä½¿ç”¨ set.union å’Œ list() æ¥åˆå¹¶å¹¶å»é‡
            vars_to_process_2d = list(set(missing_other_vars_2d).union(force_replace_vars))

            # 4. æ£€æŸ¥æ˜¯å¦æ— äº‹å¯åš
            if not vars_to_process_1d and not vars_to_process_2d:
                print("  æ‰€æœ‰å˜é‡å‡å·²å­˜åœ¨ä¸”æ— éœ€å¼ºåˆ¶æ›¿æ¢ï¼Œè·³è¿‡ã€‚")
                continue

            # 5. æ‰“å°æœ¬æ¬¡å°†è¦å¤„ç†çš„å˜é‡æ¸…å• (æ–°çš„æ‰“å°é€»è¾‘)
            print("  ----------------------------------------")
            if force_replace_vars.intersection(vars_to_process_2d):
                print(f"  å¼ºåˆ¶æ›¿æ¢å˜é‡: {list(force_replace_vars.intersection(vars_to_process_2d))}")
            if missing_other_vars_1d:
                print(f"  å¾…æ–°å¢å˜é‡ (1D): {missing_other_vars_1d}")
            if missing_other_vars_2d:
                print(f"  å¾…æ–°å¢å˜é‡ (2D): {missing_other_vars_2d}")
            print("  ----------------------------------------")

            # ========== æ›¿æ¢åˆ°è¿™é‡Œç»“æŸ ==========
            # # æ‰“å°æœ¬æ¬¡å°†æ–°å¢çš„å˜é‡æ¸…å•
            # if missing_vars_1d:
            #     print(f"  å¾…æ–°å¢å˜é‡ï¼ˆ1Dï¼‰: {missing_vars_1d}")
            # if missing_vars_2d:
            #     print(f"  å¾…æ–°å¢å˜é‡ï¼ˆ2Dï¼‰: {missing_vars_2d}")

            df_aug = augment_dataframe_with_vars(
                df=df,
                ds_restart=ds_restart,
                ds_special_p_restart=ds_special_p_restart,
                ds_r_list=ds_r_list,
                restart_tree=restart_tree,
                restart_coords=restart_coords,
                col_index_map=col_index_map,
                pft_index_map=pft_index_map,
                # vars_1d=missing_vars_1d,
                # vars_2d=missing_vars_2d,
                vars_1d=vars_to_process_1d, # <--- ç¡®è®¤ä¿®æ”¹
                vars_2d=vars_to_process_2d, # <--- ç¡®è®¤ä¿®æ”¹
                special_p_vars=Config.SPECIAL_P_VARS
            )

            base_name = os.path.basename(fp)
            # è¾“å‡ºå‘½åéµå¾ªå¢å¼ºå‰ç¼€: enhanced_1_training_data_batch_XX.pkl
            out_name = f"{Config.ENHANCED_PREFIX}{base_name}"
            out_path = os.path.join(args.output_dir, out_name)
            df_aug.to_pickle(out_path)
            print(f"  å·²ä¿å­˜: {out_path}")
    finally:
        print("å…³é—­æ‰€æœ‰ NetCDF æ–‡ä»¶ ...")
        ds_restart.close()
        ds_special_p_restart.close()
        for ds in ds_r_list:
            try:
                ds.close()
            except Exception:
                pass

    print("å…¨éƒ¨å®Œæˆã€‚")


if __name__ == "__main__":
    main()

